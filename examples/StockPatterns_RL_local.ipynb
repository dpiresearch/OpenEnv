{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc95ec4-668b-40b3-99c0-76abc39becb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, importlib.util\n",
    "!pip install --upgrade -qqq uv\n",
    "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
    "    try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "    except: get_numpy = \"numpy\"\n",
    "    !uv pip install -qqq \\\n",
    "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers==4.56.2\" trackio \\\n",
    "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "        git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels\n",
    "elif importlib.util.find_spec(\"unsloth\") is None:\n",
    "    !uv pip install -qqq unsloth trackio\n",
    "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "472dd752-c804-4dd2-bca1-a117c4e5d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qqq fastapi uvicorn requests\n",
    "!git clone https://github.com/dpiresearch/OpenEnv.git\n",
    "%cd OpenEnv\n",
    "import subprocess, sys, os\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, './src')\n",
    "working_directory = str(Path.cwd().parent.absolute() / \"OpenEnv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4b9ba1-3545-4fec-9bfa-c58590bbec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/AIAC/OpenEnv\n",
      "README.md    atari_env\tcoding_env  finrl_env  openspiel_env\n",
      "__init.py__  chat_env\techo_env    git_env    sumo_rl_env\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls src/envs\n",
    "!touch src/envs/__init.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064a5915-dc08-470b-94de-d6ada9c68539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "#### Unsloth: `hf_xet==1.1.10` and `ipykernel>6.30.1` breaks progress bars. Disabling for now in XET.\n",
      "#### Unsloth: To re-enable progress bars, please downgrade to `ipykernel==6.30.1` or wait for a fix to\n",
      "https://github.com/huggingface/xet-core/issues/526\n",
      "INFO 10-28 22:21:20 [__init__.py:225] Automatically detected platform rocm.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "Unsloth: AMD currently is not stable with 4bit bitsandbytes. Disabling for now.\n",
      "==((====))==  Unsloth 2025.10.9: Fast Gpt_Oss patching. Transformers: 4.56.2. vLLM: 0.11.1rc3.dev39+gf417746ad.rocm700.\n",
      "   \\\\   /|    . Num GPUs = 1. Max memory: 191.688 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0a0+git1c57644. ROCm Toolkit: 7.0.51831-a3e329ad8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efa1784ab1146c5a5e045dd07a565c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 768 # Can increase for longer RL output\n",
    "lora_rank = 4        # Larger rank = smarter, but slower\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gpt-oss-20b-BF16\",\n",
    "    load_in_4bit = False,\n",
    "    max_seq_length = max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14aa1423-4d5e-4965-9d99-2f60f0d210f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d6baedd-d128-40c3-b146-3d84b5712530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'OpenEnv'\n",
      "/workspace/AIAC/OpenEnv/OpenEnv/src\n",
      "[Errno 2] No such file or directory: 'src'\n",
      "/workspace/AIAC/OpenEnv/OpenEnv/src\n",
      "/workspace/AIAC/OpenEnv/OpenEnv/src\n",
      "ls: cannot access 'src/envs': No such file or directory\n",
      "touch: cannot touch 'src/envs/__init.py__': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Make it so we can recognize the new OpenEnv directory holding the new environments\n",
    "#\n",
    "%cd OpenEnv\n",
    "%cd src\n",
    "!pwd\n",
    "!ls src/envs\n",
    "!touch src/envs/__init.py__\n",
    "from envs.stock_patterns_env import StockPatternEnv\n",
    "from envs.stock_patterns_env.models import StockPatternAction, StockPatternObservation, StockPatternState\n",
    "from envs.stock_patterns_env.server.stock_patterns_environment import StockPatternEnvironment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e796a3ee-8a4f-468f-9327-9b11f7a7861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d0c9651-5c97-4eec-ab52-2d77ab7b5f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleRLAgent:\n",
    "    \"\"\"\n",
    "    Ultra-simple Q-learning agent for demonstration.\n",
    "    \n",
    "    Uses a simple discretization strategy and tabular Q-learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount=0.95, epsilon=0.2):\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount\n",
    "        self.epsilon = epsilon\n",
    "        self.q_values = defaultdict(lambda: [0.0, 0.0, 0.0])  # [Hold, Buy, Sell]\n",
    "    \n",
    "    def get_state(self, prices, position):\n",
    "        \"\"\"\n",
    "        Convert observation to simple state representation.\n",
    "        \n",
    "        Uses last 3 price movements (up/down) and current position.\n",
    "        \"\"\"\n",
    "        if len(prices) < 4:\n",
    "            return (0, 0, 0, position)\n",
    "        \n",
    "        # Get last 3 price changes\n",
    "        recent = prices[-4:]\n",
    "        movements = []\n",
    "        for i in range(1, 4):\n",
    "            if recent[i] > recent[i-1]:\n",
    "                movements.append(1)  # Up\n",
    "            else:\n",
    "                movements.append(0)  # Down\n",
    "        \n",
    "        return tuple(movements + [position])\n",
    "    \n",
    "    def select_action(self, state, position):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        # Determine valid actions based on position\n",
    "        if position == 0:\n",
    "            valid_actions = [0, 1]  # Hold or Buy\n",
    "        else:\n",
    "            valid_actions = [0, 2]  # Hold or Sell\n",
    "        \n",
    "        # Epsilon-greedy\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(valid_actions)\n",
    "        \n",
    "        # Choose best Q-value among valid actions\n",
    "        q_vals = self.q_values[state]\n",
    "        return max(valid_actions, key=lambda a: q_vals[a])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-learning rule.\"\"\"\n",
    "        current_q = self.q_values[state][action]\n",
    "        \n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            max_next_q = max(self.q_values[next_state])\n",
    "            target_q = reward + self.gamma * max_next_q\n",
    "        \n",
    "        # Q-learning update\n",
    "        self.q_values[state][action] += self.lr * (target_q - current_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e26512c5-9ebc-4a39-bc27-d2c5b72bf458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_rl_training(episodes=50, max_steps=80, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the RL training loop.\n",
    "    \n",
    "    Args:\n",
    "        episodes: Number of training episodes\n",
    "        max_steps: Maximum steps per episode\n",
    "        verbose: Print progress\n",
    "    \"\"\"\n",
    "    print(\"🎓 Simple RL Training on Stock Patterns\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create environment (local, no server needed)\n",
    "    env = StockPatternEnvironment(\n",
    "        pattern_name=None,  # Random patterns\n",
    "        difficulty=0.4,     # Medium difficulty\n",
    "    )\n",
    "    \n",
    "    # Create agent\n",
    "    agent = SimpleRLAgent(\n",
    "        learning_rate=0.1,\n",
    "        discount=0.95,\n",
    "        epsilon=0.2,\n",
    "    )\n",
    "    \n",
    "    # Track performance\n",
    "    all_rewards = []\n",
    "    all_returns = []\n",
    "    profitable_episodes = 0\n",
    "    \n",
    "    print(f\"Training for {episodes} episodes...\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment for new episode\n",
    "        obs = env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        \n",
    "        # Get initial state\n",
    "        state = agent.get_state(obs.prices, obs.position)\n",
    "        \n",
    "        # Run episode\n",
    "        while not obs.done and step < max_steps:\n",
    "            # Agent selects action\n",
    "            action = agent.select_action(state, obs.position)\n",
    "            \n",
    "            # Execute action in environment\n",
    "            obs = env.step(StockPatternAction(\n",
    "                action_type=\"trade\",\n",
    "                trade_action=action\n",
    "            ))\n",
    "            \n",
    "            # Get reward\n",
    "            reward = obs.reward or 0.0\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Get next state\n",
    "            next_state = agent.get_state(obs.prices, obs.position)\n",
    "            \n",
    "            # Learn from experience\n",
    "            agent.learn(state, action, reward, next_state, obs.done)\n",
    "            \n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        \n",
    "        # Track statistics\n",
    "        final_return = env.state.total_return\n",
    "        all_rewards.append(episode_reward)\n",
    "        all_returns.append(final_return)\n",
    "        \n",
    "        if final_return > 0:\n",
    "            profitable_episodes += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (episode + 1) % 10 == 0:\n",
    "            avg_reward = sum(all_rewards[-10:]) / min(10, len(all_rewards))\n",
    "            avg_return = sum(all_returns[-10:]) / min(10, len(all_returns))\n",
    "            win_rate = profitable_episodes / (episode + 1) * 100\n",
    "            \n",
    "            print(f\"Episode {episode + 1}/{episodes}\")\n",
    "            print(f\"  Avg Reward (last 10): {avg_reward:.2f}\")\n",
    "            print(f\"  Avg Return (last 10): {avg_return:.2f}%\")\n",
    "            print(f\"  Win Rate: {win_rate:.1f}%\")\n",
    "            print(f\"  States learned: {len(agent.q_values)}\")\n",
    "            print()\n",
    "    \n",
    "    # Final statistics\n",
    "    print(\"=\" * 60)\n",
    "    print(\"✅ Training Complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    avg_reward = sum(all_rewards) / len(all_rewards)\n",
    "    avg_return = sum(all_returns) / len(all_returns)\n",
    "    win_rate = profitable_episodes / episodes * 100\n",
    "    \n",
    "    print(f\"Overall Performance:\")\n",
    "    print(f\"  Average Reward: {avg_reward:.2f}\")\n",
    "    print(f\"  Average Return: {avg_return:.2f}%\")\n",
    "    print(f\"  Win Rate: {win_rate:.1f}% ({profitable_episodes}/{episodes})\")\n",
    "    print(f\"  States Learned: {len(agent.q_values)}\")\n",
    "    print()\n",
    "    \n",
    "    if avg_return > 0:\n",
    "        print(\"💰 Agent learned to make profitable trades on average!\")\n",
    "    else:\n",
    "        print(\"📉 Agent needs more training or hyperparameter tuning.\")\n",
    "    \n",
    "    # Show some learned Q-values\n",
    "    print(\"\\n📚 Sample Learned Q-Values:\")\n",
    "    print(\"   State          ->  [Hold,  Buy, Sell]\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, (state, q_vals) in enumerate(list(agent.q_values.items())[:5]):\n",
    "        print(f\"   {state} -> [{q_vals[0]:6.2f}, {q_vals[1]:6.2f}, {q_vals[2]:6.2f}]\")\n",
    "    print()\n",
    "    \n",
    "    return {\n",
    "        'rewards': all_rewards,\n",
    "        'returns': all_returns,\n",
    "        'win_rate': win_rate,\n",
    "        'agent': agent,\n",
    "    }\n",
    "\n",
    "\n",
    "def test_trained_agent(agent, episodes=10):\n",
    "    \"\"\"\n",
    "    Test the trained agent on new episodes.\n",
    "    \n",
    "    Args:\n",
    "        agent: Trained agent\n",
    "        episodes: Number of test episodes\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎯 Testing Trained Agent\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    env = StockPatternEnvironment(difficulty=0.4)\n",
    "    \n",
    "    # Disable exploration for testing\n",
    "    old_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    test_returns = []\n",
    "    wins = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        obs = env.reset()\n",
    "        state = agent.get_state(obs.prices, obs.position)\n",
    "        step = 0\n",
    "        \n",
    "        while not obs.done and step < 80:\n",
    "            action = agent.select_action(state, obs.position)\n",
    "            obs = env.step(StockPatternAction(action_type=\"trade\", trade_action=action))\n",
    "            state = agent.get_state(obs.prices, obs.position)\n",
    "            step += 1\n",
    "        \n",
    "        final_return = env.state.total_return\n",
    "        test_returns.append(final_return)\n",
    "        \n",
    "        if final_return > 0:\n",
    "            wins += 1\n",
    "        \n",
    "        pattern_name = env.state.pattern_name\n",
    "        print(f\"  Test {ep+1}: Return={final_return:6.2f}%, Pattern={pattern_name}\")\n",
    "    \n",
    "    # Restore epsilon\n",
    "    agent.epsilon = old_epsilon\n",
    "    \n",
    "    avg_return = sum(test_returns) / len(test_returns)\n",
    "    win_rate = wins / episodes * 100\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Average Return: {avg_return:.2f}%\")\n",
    "    print(f\"  Win Rate: {win_rate:.1f}%\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e05889e-255a-4acb-93ba-e658d8334b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 Simple RL Training on Stock Patterns\n",
      "============================================================\n",
      "Training for 50 episodes...\n",
      "\n",
      "Episode 10/50\n",
      "  Avg Reward (last 10): 2.92\n",
      "  Avg Return (last 10): 2.74%\n",
      "  Win Rate: 60.0%\n",
      "  States learned: 16\n",
      "\n",
      "Episode 20/50\n",
      "  Avg Reward (last 10): 8.54\n",
      "  Avg Return (last 10): 8.89%\n",
      "  Win Rate: 75.0%\n",
      "  States learned: 16\n",
      "\n",
      "Episode 30/50\n",
      "  Avg Reward (last 10): 9.07\n",
      "  Avg Return (last 10): 5.98%\n",
      "  Win Rate: 70.0%\n",
      "  States learned: 16\n",
      "\n",
      "Episode 40/50\n",
      "  Avg Reward (last 10): 12.85\n",
      "  Avg Return (last 10): 13.61%\n",
      "  Win Rate: 72.5%\n",
      "  States learned: 16\n",
      "\n",
      "Episode 50/50\n",
      "  Avg Reward (last 10): -0.13\n",
      "  Avg Return (last 10): -0.85%\n",
      "  Win Rate: 64.0%\n",
      "  States learned: 16\n",
      "\n",
      "============================================================\n",
      "✅ Training Complete!\n",
      "============================================================\n",
      "Overall Performance:\n",
      "  Average Reward: 6.65\n",
      "  Average Return: 6.07%\n",
      "  Win Rate: 64.0% (32/50)\n",
      "  States Learned: 16\n",
      "\n",
      "💰 Agent learned to make profitable trades on average!\n",
      "\n",
      "📚 Sample Learned Q-Values:\n",
      "   State          ->  [Hold,  Buy, Sell]\n",
      "--------------------------------------------------\n",
      "   (0, 1, 1, 0) -> [  2.09,   3.85,   0.00]\n",
      "   (1, 1, 0, 0) -> [  2.66,   1.49,   0.00]\n",
      "   (1, 0, 0, 0) -> [  2.52,   0.86,   0.00]\n",
      "   (0, 0, 1, 0) -> [  3.44,   1.38,   0.00]\n",
      "   (0, 1, 0, 0) -> [  2.51,   3.31,   0.00]\n",
      "\n",
      "\n",
      "============================================================\n",
      "🎯 Testing Trained Agent\n",
      "============================================================\n",
      "  Test 1: Return= 22.70%, Pattern=double_top\n",
      "  Test 2: Return= 41.16%, Pattern=flag\n",
      "  Test 3: Return= 28.78%, Pattern=double_bottom\n",
      "  Test 4: Return= 13.70%, Pattern=flag\n",
      "  Test 5: Return=-24.61%, Pattern=flag\n",
      "  Test 6: Return= 14.18%, Pattern=symmetrical_triangle\n",
      "  Test 7: Return= 12.05%, Pattern=double_bottom\n",
      "  Test 8: Return= 13.10%, Pattern=head_and_shoulders\n",
      "  Test 9: Return= 35.13%, Pattern=head_and_shoulders\n",
      "  Test 10: Return= 23.43%, Pattern=ascending_triangle\n",
      "\n",
      "Test Results:\n",
      "  Average Return: 17.96%\n",
      "  Win Rate: 90.0%\n",
      "\n",
      "============================================================\n",
      "🎉 All Done!\n",
      "============================================================\n",
      "\n",
      "Key Takeaways:\n",
      "• The agent learns Q-values for different price patterns\n",
      "• Exploration (epsilon) helps discover good strategies\n",
      "• The agent learns to buy low and sell high\n",
      "• More episodes → better performance (usually)\n",
      "\n",
      "Next Steps:\n",
      "• Try adjusting learning_rate, discount, epsilon\n",
      "• Increase episodes for better convergence\n",
      "• Add more features to state representation\n",
      "• Implement more sophisticated RL algorithms (DQN, PPO)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main function.\"\"\"\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Simple RL training on Stock Patterns'\n",
    ")\n",
    "parser.add_argument('--episodes', type=int, default=50,\n",
    "                   help='Number of training episodes (default: 50)')\n",
    "parser.add_argument('--test-episodes', type=int, default=10,\n",
    "                   help='Number of test episodes (default: 10)')\n",
    "parser.add_argument('--max-steps', type=int, default=80,\n",
    "                   help='Max steps per episode (default: 80)')\n",
    "print(\"Before parsing\")\n",
    "args = parser.parse_args()\n",
    "print(\"After parsing\")\n",
    "\"\"\"\n",
    "try:\n",
    "    # Run training\n",
    "    '''\n",
    "    results = run_rl_training(\n",
    "            episodes=args.episodes,\n",
    "            max_steps=args.max_steps,\n",
    "            verbose=True\n",
    "        )\n",
    "    '''\n",
    "    results = run_rl_training(\n",
    "        episodes=50,\n",
    "        max_steps=80,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Test trained agent\n",
    "#        test_trained_agent(results['agent'], episodes=args.test_episodes)\n",
    "    test_trained_agent(results['agent'], episodes=10)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"🎉 All Done!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nKey Takeaways:\")\n",
    "    print(\"• The agent learns Q-values for different price patterns\")\n",
    "    print(\"• Exploration (epsilon) helps discover good strategies\")\n",
    "    print(\"• The agent learns to buy low and sell high\")\n",
    "    print(\"• More episodes → better performance (usually)\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"• Try adjusting learning_rate, discount, epsilon\")\n",
    "    print(\"• Increase episodes for better convergence\")\n",
    "    print(\"• Add more features to state representation\")\n",
    "    print(\"• Implement more sophisticated RL algorithms (DQN, PPO)\")\n",
    "    print()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n⚠️  Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265a8be-0c26-4a7d-acb6-65b91ec7cd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
